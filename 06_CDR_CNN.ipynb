{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---This program is used to forecast 1 week CDRs based on 55 previous days' data\n",
    "\n",
    "#---Install these bibs so the program works properly\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d10ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Loading Cell_ID vector, whose elements tell from which ID the matrix_TS row is the corresponding time-series\n",
    "\n",
    "Cell_ID = loadtxt('ID_labels.csv', delimiter=',') \n",
    "df = pd.read_csv('matrixTS.csv',header=None)\n",
    "df.head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c53f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Defines the Cell ID to be forecasted, size of training data\n",
    "\n",
    "days = 62 #Train and validation sizes need to be changed if this value is tunned\n",
    "matrix = df.to_numpy()\n",
    "\n",
    "#-- This part is to get the cell ID time-series of IDs that surround 1051 [1050, 1052, 1150, 1151, 1152]\n",
    "ID_1051=60\n",
    "cells = [1050, 1052, 1150, 1151, 1152]\n",
    "pos = []\n",
    "\n",
    "for item in cells:\n",
    "    for i in range (0,len(Cell_ID)):\n",
    "        if item == Cell_ID[i]:\n",
    "            pos.append(i)\n",
    "            break\n",
    "print('1050, 1052, 1150, 1151, 1152:',Cell_ID[pos[0]], Cell_ID[pos[1]], Cell_ID[pos[2]], Cell_ID[pos[3]], Cell_ID[pos[4]])\n",
    "\n",
    "TS_950 = np.array(loadtxt('CellID950.csv', delimiter=','))\n",
    "TS_951 = np.array(loadtxt('CellID951.csv', delimiter=','))\n",
    "TS_952 = np.array(loadtxt('CellID952.csv', delimiter=','))\n",
    "\n",
    "TS_1050 = matrix[pos[0]][:]\n",
    "TS_1051 = matrix[ID_1051][:] #-This is the one to predict\n",
    "TS_1052 = matrix[pos[1]][:]\n",
    "TS_1150 = matrix[pos[2]][:]\n",
    "TS_1151 = matrix[pos[3]][:]\n",
    "TS_1152 = matrix[pos[4]][:]\n",
    "\n",
    "#--Spliting the data set into Training/Validating/Testing\n",
    "#--Standardizing the time-series\n",
    "train_size = 24*50 \n",
    "validation_size = 24*5\n",
    "\n",
    "mean_TS_950 = TS_950[:train_size].mean() #--Considering just the training part\n",
    "std_TS_950 = TS_950[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_951 = TS_951[:train_size].mean() #--Considering just the training part\n",
    "std_TS_951 = TS_951[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_952 = TS_952[:train_size].mean() #--Considering just the training part\n",
    "std_TS_952 = TS_952[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_1050 = TS_1050[:train_size].mean() #--Considering just the training part\n",
    "std_TS_1050 = TS_1050[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_1051 = TS_1051[:train_size].mean() #--Considering just the training part\n",
    "std_TS_1051 = TS_1051[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_1052 = TS_1052[:train_size].mean() #--Considering just the training part\n",
    "std_TS_1052 = TS_1052[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_1150 = TS_1150[:train_size].mean() #--Considering just the training part\n",
    "std_TS_1150 = TS_1150[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_1151 = TS_1151[:train_size].mean() #--Considering just the training part\n",
    "std_TS_1151 = TS_1151[:train_size].std() #--Considering just the training part\n",
    "\n",
    "mean_TS_1152 = TS_1152[:train_size].mean() #--Considering just the training part\n",
    "std_TS_1152 = TS_1152[:train_size].std() #--Considering just the training part\n",
    "\n",
    "\n",
    "Norm_TS_950 = (TS_950 - mean_TS_950) / std_TS_950\n",
    "Norm_TS_951 = (TS_951 - mean_TS_951) / std_TS_951\n",
    "Norm_TS_952 = (TS_952 - mean_TS_952) / std_TS_952\n",
    "\n",
    "Norm_TS_1050 = (TS_1050 - mean_TS_1050) / std_TS_1050\n",
    "Norm_TS_1051 = (TS_1051 - mean_TS_1051) / std_TS_1051\n",
    "Norm_TS_1052 = (TS_1052 - mean_TS_1052) / std_TS_1052\n",
    "\n",
    "Norm_TS_1150 = (TS_1150 - mean_TS_1150) / std_TS_1150\n",
    "Norm_TS_1151 = (TS_1151 - mean_TS_1151) / std_TS_1151\n",
    "Norm_TS_1152 = (TS_1152 - mean_TS_1152) / std_TS_1152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ce0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Plot of the Cell ID CDR\n",
    "hour = np.linspace(0,24*days,24*days)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Traffic load of Cell IDs') \n",
    "plt.plot(hour,TS_1051,'b',label=\"Cell ID: %d\" % (Cell_ID[ID_1051]))\n",
    "plt.plot(hour,TS_950,'r',label=\"Cell ID: 950\")\n",
    "plt.legend(loc='upper right',fontsize=13)\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"CDR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ad46e",
   "metadata": {},
   "source": [
    "## Constructing the 3D tensor and respectives Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec66c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = []\n",
    "\n",
    "for i in range(0,len(TS_950)):\n",
    "    tensor.append([[Norm_TS_1150[i], Norm_TS_1151[i], Norm_TS_1152[i]], \n",
    "                        [Norm_TS_1050[i], Norm_TS_1051[i], Norm_TS_1052[i]],\n",
    "                        [Norm_TS_950[i], Norm_TS_951[i], Norm_TS_952[i]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca275d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---This function creates the sliding window to construct the training data set. \n",
    "\n",
    "def df_to_X_y(df,window_size):\n",
    "    df_as_np = df\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(tensor)-window_size):\n",
    "        row = [a for a in df_as_np[i:i+window_size]] \n",
    "        X.append(row)\n",
    "        y.append(df_as_np[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#-X contains 1464 tensors with 24 matrices (24 hours) of dimension 3x3 (IDs surrounding ID 1051- see Figure in the Survey Paper)\n",
    "#-y contains 1464 matrices of dimension 3x3 used in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 24\n",
    "X,y = df_to_X_y(tensor,WINDOW_SIZE)\n",
    "print('Initial Shape', X.shape, y.shape)\n",
    "\n",
    "y_flatten = []\n",
    "for item in y:\n",
    "    y_flatten.append(item.flatten())\n",
    "\n",
    "y_flatten = np.array(y_flatten)\n",
    "print('Final Shape', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Reshape tensor X\n",
    "X = tf.expand_dims(X,axis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252f57e",
   "metadata": {},
   "source": [
    "## Splitting data to training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ae6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:train_size], y_flatten[:train_size]\n",
    "X_val, y_val = X[train_size:(train_size+validation_size)], y_flatten[train_size:(train_size+validation_size)]\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce7293",
   "metadata": {},
   "source": [
    "## Constructing Simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "#-Comment or uncomment lines to insert/remove layers of the CNN\n",
    "#-Change the number of neurons inside each layers\n",
    "\n",
    "d=1 # Kernel size in time-dimension\n",
    "model1.add(layers.Conv3D(32, (d, 2, 2), activation='relu', padding='same', input_shape=(24, 3, 3, 1)))\n",
    "model1.add(layers.AveragePooling3D((2, 2, 2),padding='same'))\n",
    "\n",
    "model1.add(layers.Conv3D(32, (d, 2, 2), activation='relu',padding='valid'))\n",
    "model1.add(layers.AveragePooling3D((2, 1, 1),padding='valid'))\n",
    "\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(Dense(32,'relu'))\n",
    "model1.add(Dense(9,'linear'))\n",
    "\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e18848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--Model Training\n",
    "\n",
    "#-Set the hyperparemeters according to your application: learning_rate, epochs, batch size\n",
    "model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])\n",
    "model1.fit(X_train,y_train, validation_data=(X_val,y_val),epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Predictions: the prediction of one step becomes input for the next one\n",
    "\n",
    "input_pred_tensor = np.array(X_val[X_val.shape[0]-1]) # last position of validation tensor\n",
    "input_pred_tensor = tf.expand_dims(input_pred_tensor,axis=0)\n",
    "prediction_ID_1051 = []\n",
    "\n",
    "N_day_pred = 7\n",
    "for i in range(0,24*N_day_pred):\n",
    "    prediction = model1.predict(input_pred_tensor)\n",
    "    new_values = [[[prediction[0][0]],[prediction[0][1]],[prediction[0][2]]],\n",
    "                 [ [prediction[0][3]],[prediction[0][4]],[prediction[0][5]]],\n",
    "                 [ [prediction[0][6]],[prediction[0][7]],[prediction[0][8]]]]\n",
    "    \n",
    "    prediction_ID_1051.append(prediction[0][4])\n",
    "    \n",
    "    #Erase first position and add new values to the last one\n",
    "    input_pred_tensor = np.array(input_pred_tensor[0][1:])\n",
    "    input_pred_tensor = input_pred_tensor.tolist()\n",
    "    input_pred_tensor.append(new_values)\n",
    "    input_pred_tensor = np.array(input_pred_tensor)\n",
    "    input_pred_tensor = tf.expand_dims(input_pred_tensor,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f256d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "prediction_ID_1051 = np.array(prediction_ID_1051)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Prediction for Cell ID = %d (Standardized)' % (Cell_ID[60])) \n",
    "plt.plot(hour[(train_size+validation_size):],Norm_TS_1051[(train_size+validation_size):],'b',label=\"Observed\")\n",
    "plt.plot(hour[(train_size+validation_size):], prediction_ID_1051,'r',label=\"Predicted\")\n",
    "plt.legend(loc='upper right',fontsize=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Non-standardized prediction\n",
    "NN_prediction_ID_1051 = prediction_ID_1051*std_TS_1051+mean_TS_1051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b92e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Plotting the predictions\n",
    "prediction_ID_1051 = np.array(prediction_ID_1051)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Prediction for Cell ID = %d' % (Cell_ID[60])) \n",
    "plt.plot(hour[(train_size+validation_size):],TS_1051[(train_size+validation_size):],'b',label=\"Observed\")\n",
    "plt.plot(hour[(train_size+validation_size):], NN_prediction_ID_1051,'r',label=\"Predicted\")\n",
    "plt.legend(loc='upper right',fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3069e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Accuracy results\n",
    "\n",
    "MAE = mean_absolute_error(NN_prediction_ID_1051,TS_1051[(train_size+validation_size):])\n",
    "MSE = mean_squared_error(NN_prediction_ID_1051,TS_1051[(train_size+validation_size):])\n",
    "print('MAE:', \"%.3f\" % MAE)\n",
    "print('MSE:', \"%.3f\" % MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ef13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f832a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1deff45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58896e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27123ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b896c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34aaf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e9105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cb0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fc274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0169d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa68c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
